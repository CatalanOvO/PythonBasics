### 十大回归类算法模型

常见的回归类算法如下：

1. 线性回归
2. Ridge回归
3. Lasso回归
4. 弹性网络回归
5. 多项式回归
6. 决策树回归
7. 随机森林回归
8. 支持向量回归
9. K-means回归
10. 梯度提升回归



### 1. 线性回归

>线性回归是一种用于描述两个或多个变量之间线性关系的统计模型。假设 $y$ 是响应变量（目标变量），$x_1,x_2,\cdots, x_n$ 是解释变量（特征），线性回归模型通过拟合一条直线来预测目标变量

#### 原理：

- 基本假设： $y=\beta_{0}+\beta_{1}x_1+\beta_{2}x_2+\cdots+\beta_{n}x_n+\epsilon$
- 其中，$\beta_0$ 是截距，$\beta_1,\beta_2,\cdots,\beta_n$ 是回归系数，$\epsilon$ 是误差项（即残差），假设其服从正态分布。

#### 核心公式：

 线性回归的目标是通过最小化均方误差（MSE）来拟合参数：

$$MSE=\frac{1}{m}\sum\limits_{i=1}^{m}(y_i-\hat{y_i})^2 = \frac{1}{m}\sum\limits_{i=1}^{m}(y_i-(\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\cdots+\beta_{n}x_{in}))^2$$

其中，$m$ 是样本数量，$y_i$ 是实际值，$\hat{y_i}$ 是预测值。



#### 公式推导

最小化 $MSE$ 的损失函数可以使用 **梯度下降法** 。损失函数 $L(\beta)$ 表示为：$L(\beta)=\frac{1}{m}\sum\limits_{i=1}^m(y_i-X_i^T \beta)$

其中，$X_i$ 是输入特征向量，$\beta$ 是参数向量。对 $\beta$ 进行梯度计算：$\nabla_{\beta}L(\beta)=-\frac{2}{m}\sum\limits_{i=1}^{m}X_i(y_i)-X_{i}^T \beta$

更新规则为：$\beta = \beta - \alpha \nabla_{\beta} L(\beta)$，其中 $\alpha$ 是学习率。



#### 正规方程推导

对于小规模的数据集，线性回归可以直接通过求解解析解来找到最优参数。最小化 $MSE$ 时，损失函数的梯度为：

$\frac{\partial L(\beta)}{\partial \beta}=-2 X^T(y-X \beta)$，令其等于 $0$，解得：$\beta = (X^TX)^{-1}X^Ty$，其中 $X$ 是特征矩阵，$y$ 是目标变量。

详见：[矩阵求导相关运算规则](https://github.com/CatalanOvO/PythonBasics/blob/main/Regressions/MatrixDerivative.pdf)



#### [Python 实现]()





### 2. Ridge 回归

> 岭回归是线性回归的一种改进，用于处理多重共线性的问题。在普通线性回归中，如果特征之间存在较高的相关性，模型可能变得不稳定且参数估计不准确。为了解决这个问题，岭回归在损失函数中加入了 $L_2$ 正则化项，从而限制参数的大小，防止过拟合。

#### 原理

岭回归的目标是最小化带有正则化项的损失函数：$Loss=MSE+\lambda \sum\limits_{j=1}^{n} \beta_{j}^2$ 

其中，$\lambda$ 是正则化参数，用于控制模型的复杂度。如果 $\lambda$ 很大，模型的复杂度将受到严格约束，可能导致欠拟合；如果 $\lambda$ 很小，则正则化效果减弱，回归模型趋于普通的线性回归。

#### 核心公式

岭回归的目标函数为：$L(\beta) = \frac{1}{m}\sum\limits_{i=1}^m(y_i-\hat{y_i})^2+\lambda \sum\limits_{j=1}^n \beta_j^2$

其中，$\lambda$ 控制正则化项的权重。

#### 公式推导

岭回归可以通过修改线性回归的正规方程来解决。在线性回归中，参数估计公式为：$\beta = (X^TX)^{-1}X^Ty$ 

为了引入正则化，我们将目标修改为最小化：$L(\beta) = (y-X \beta)^T(y-X \beta)+\lambda \beta^T \beta$

对其求导并令其等于 $0$，可以得到更新后的解析解：$\beta = (X^TX+\lambda I)^{-1}X^Ty$，其中 $I$ 是单位矩阵，$\lambda$ 是正则化参数



#### [Python实现]()

>1. 散点图和回归曲线:
>    帮助我们查看自变量与目标变量之间的实际关系以及模型拟合的效果, 比较真实值和预测值的差异, 判断模型是否合理
>2. 残差图:
>    用于分析预测值与实际值的差异是否具有系统性的偏差, 检查模型假设是否被满足, 是否存在任何未捕捉的趋势
>3. 岭回归系数路径:
>    展示了不同正则化强度下模型系数的变化, 帮助我们直观了解正则化对模型复杂度的影响
>4. MSE曲线:
>    展示不同alpha值下模型在交叉验证中的表现, 用来选出最优的正则化参数, 帮助我们减少过拟合
>     这些图结合起来, 可以多维度地分析岭回归模型的效果, 评估模型的预测能力和正则化对结果的影响。
>





### 3. Lasso回归

> 拉索回归是另一种正则化的线性回归方法，与岭回归不同，它使用 $L_1$ 正则化项。拉索回归的优势在于它不仅可以限制模型的复杂度，还可以实现特征选择，因为 $L_1$ 正则化倾向于将某些回归系数缩为零。

#### 原理

拉索回归的损失函数为：$Loss=\frac{1}{m} \sum\limits_{i=1}^m(y_i-\hat{y_i})^2+\lambda \sum\limits_{j=1}^n|\beta_j|$

通过引入 $L_1$ 正则化项，模型会倾向于将某些系数缩小到零，从而实现自动的特征选择。

#### 核心公式

拉索回归的目标函数为：$L(\beta)=\frac{1}{m}\sum\limits_{i=1}^m(y_i-\hat{y_i})^2+\lambda \sum\limits_{j=1}^n|\beta_j|$

#### 公式推导

拉索回归没有像岭回归那样的解析解，因为 $L_1$ 正则化项的绝对值函数不可微。因此，拉索回归的参数通常通过迭代优化方法（如**坐标下降法**）求解。

##### 坐标下降法推导：

使用坐标轴下降法来求解。对于每个 $\beta_j$，我们固定其他参数，解决以下优化问题：

​						$$\beta_j \larr arg \mathop{min}\limits_{\beta}[\frac{1}{m}\sum\limits_{i=1}^m(y_i- \sum\limits_{k\neq j}\beta_kx_{ik}-\beta_jx_{ij})^2+\lambda |\beta_j|]$$

这个优化过程依赖于软阈值算法，将某些系数强制为零。



#### [Python实现]()



### 4. 弹性网络回归

> 弹性网络回归结合了岭回归的 $L_2$ 正则化和拉索回归的 $L_1$ 正则化。它可以同时处理特征选择（通过 $L_1$ 正则化）和共线性问题（通过 $L_2$ 正则化）

#### 原理

弹性网络回归的损失函数为：$Loss=\frac{1}{m}\sum\limits_{i=1}^m(y_i-\hat{y_i})^2+\lambda_1\sum\limits_{j=1}^n|\beta_j|+\lambda_2\sum\limits_{j=1}^n\beta_j^2$，且 $\lambda_1+\lambda_2=1$

通过同时使用 $L_1$ 和 $L_2$ 正则化，弹性网络可以解决拉索回归在处理多重共线性时不稳定的缺点

#### 核心公式

目标函数：$L(\beta)=\frac{1}{m}\sum\limits_{i=1}^m(y_i-\hat{y_i})^2+\lambda_1\sum\limits_{j=1}^n|\beta_j|+\lambda_2\sum\limits_{j=1}^n\beta_j^2$

#### 公式推导

弹性网络回归的解通常通过坐标轴下降法或其他迭代方法求解。因为有两个正则化项所以没有简单的解析解。



#### [Python实现]()





### 5. 多项式回归

>一种拓展的线性回归方法，通过增加特征的多项式项（如平方、立方等），捕捉非线性关系

#### 原理

假设输入特征为 $x$，多项式回归模型为：$y=\beta_0+\beta_1x+\cdots+\beta_nx^n+\epsilon$

其中 $n$ 是多项式的项数。虽然模型看似非线性，但本质上它仍然是线性回归，只是通过对输入进行非线性变换来增强表达能力。

- 可以通过增加特征维度，将原始特征 $x$ 转换为 $x,x^2,\cdots,x^n$，然后应用标准的线性回归方法。

#### 步骤

1. **选择多项式的阶数：**选择合适的多项式阶数 $n$ 是模型拟合的关键。阶数过低可能会导致欠拟合，阶数过高则会导致过拟合。
2. **构建多项式特征：**将输入特征拓展为多项式特征。例如对于一个一维特征 $x$，构建的特征矩阵为 $\begin{bmatrix} x & x^2 & x^3 & \cdots & x^n \end{bmatrix}$。
3. **拟合模型：**使用线性回归方法在多项式特征上进行拟合。
4. **评估模型：**MSE 等标准。

#### 

#### [Python实现]()





### 6. 回归

>

#### 原理

#### 核心公式

#### 公式推导

#### [Python实现]()





### 7. 回归

>

#### 原理

#### 核心公式

#### 公式推导

#### [Python实现]()





### 8. 回归

>

#### 原理

#### 核心公式

#### 公式推导

#### [Python实现]()





### 9. 回归

>

#### 原理

#### 核心公式

#### 公式推导

#### [Python实现]()





### 10. 回归

>

#### 原理

#### 核心公式

#### 公式推导

#### [Python实现]()
